{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7549227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "534e9c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "\n",
    "    num_RX_ant = None\n",
    "    num_TX_ant = None\n",
    "    num_scatterers = None\n",
    "    lam = None\n",
    "    Ant_dist = None\n",
    "    TX_locs = None\n",
    "    RX_locs = None\n",
    "    SC_locs = None\n",
    "    rand_ph = None\n",
    "\n",
    "class pair():\n",
    "    def __init__(self, N, RX_idx, TX_idx):\n",
    "        self.TX_idx = TX_idx\n",
    "        self.RX_idx = RX_idx\n",
    "        self.num_RX_ant = N.num_RX_ant\n",
    "        self.num_TX_ant = N.num_TX_ant\n",
    "        self.num_scatterers = N.num_scatterers\n",
    "        self.lam = N.lam\n",
    "        self.Ant_dist = N.Ant_dist\n",
    "        self.TX_locs = N.TX_locs\n",
    "        self.RX_locs = N.RX_locs\n",
    "        self.SC_locs = N.SC_locs\n",
    "        self.rand_ph = N.rand_ph\n",
    "        self.T_locs = [[i*self.Ant_dist+self.TX_locs[self.TX_idx][0], self.TX_locs[self.TX_idx][1]]  for i in range(self.num_TX_ant)]\n",
    "        self.R_locs = [[i*self.Ant_dist+self.RX_locs[self.RX_idx][0], self.RX_locs[self.RX_idx][1]]  for i in range(self.num_RX_ant)]\n",
    "        self.S_locs = [i for i in torch.from_numpy(self.SC_locs[self.RX_idx, self.TX_idx])]\n",
    "\n",
    "    def calculate_Bt(self):\n",
    "        Bt = torch.zeros(self.num_TX_ant, self.num_scatterers[self.RX_idx, self.TX_idx]+1, dtype=torch.complex64)\n",
    "        # qT = (RX_locs[self.RX_idx] - TX_locs[self.TX_idx])\n",
    "        qT = (self.RX_locs[self.RX_idx] - self.TX_locs[self.TX_idx])/torch.norm((self.RX_locs[self.RX_idx] - self.TX_locs[self.TX_idx]))\n",
    "        D = torch.tensor(self.T_locs) - torch.tile(self.TX_locs[self.TX_idx], (self.num_TX_ant, 1))\n",
    "        Bt[:, 0] = torch.exp(((-2*torch.pi*1j)/self.lam)*(torch.matmul(qT, torch.transpose(D, 0, 1))))\n",
    "        for i, S_loc in enumerate(self.S_locs):\n",
    "            # qT = (S_loc - TX_locs[self.TX_idx])\n",
    "            qT = (S_loc - self.TX_locs[self.TX_idx])/torch.norm((S_loc - self.TX_locs[self.TX_idx]))\n",
    "            Bt[:, i+1] = torch.exp(((-2*torch.pi*1j)/self.lam)*(torch.matmul(qT, torch.transpose(D, 0, 1))))\n",
    "        return Bt\n",
    "\n",
    "    def calculate_Br(self):\n",
    "        Br = torch.zeros(self.num_RX_ant, self.num_scatterers[self.RX_idx, self.TX_idx]+1, dtype=torch.complex64)\n",
    "        # qR = (RX_locs[self.RX_idx] - TX_locs[self.TX_idx])\n",
    "        qR = (self.RX_locs[self.RX_idx] - self.TX_locs[self.TX_idx])/torch.norm((self.RX_locs[self.RX_idx] - self.TX_locs[self.TX_idx]))\n",
    "        D = torch.tensor(self.R_locs) - torch.tile(self.RX_locs[self.RX_idx], (self.num_RX_ant, 1))\n",
    "        Br[:, 0] = torch.exp(((2*torch.pi*1j)/self.lam)*(torch.matmul(qR, torch.transpose(D, 0, 1))))\n",
    "        for i, S_loc in enumerate(self.S_locs):\n",
    "            # qR = (S_loc - RX_locs[self.RX_idx])\n",
    "            qR = (self.RX_locs[self.RX_idx] - S_loc)/torch.norm((S_loc - self.RX_locs[self.RX_idx]))\n",
    "            Br[:, i+1] = torch.exp(((2*torch.pi*1j)/self.lam)*(torch.matmul(qR, torch.transpose(D, 0, 1))))\n",
    "        return Br\n",
    "\n",
    "    def calculate_A(self, L = None):\n",
    "        A = torch.zeros(len(self.S_locs)+1, len(self.S_locs)+1, dtype=torch.complex64)\n",
    "        r = torch.norm((self.RX_locs[self.RX_idx] - self.TX_locs[self.TX_idx]))\n",
    "        A[0, 0] = (torch.exp(-2*torch.pi*(r/self.lam)*1j))/r\n",
    "        if self.rand_ph:\n",
    "            for i, S_loc in enumerate(self.S_locs):\n",
    "                r = torch.norm((S_loc - self.TX_locs[self.TX_idx])) + torch.norm((self.RX_locs[self.RX_idx] - S_loc))\n",
    "                random_phase = torch.rand(1) * 2 * torch.pi\n",
    "                A[i+1, i+1] = ((torch.exp(-2*torch.pi*(r/self.lam)*1j))*(torch.exp(random_phase*1j)))/r\n",
    "        else:\n",
    "            for i, S_loc in enumerate(self.S_locs):\n",
    "                r = torch.norm((S_loc - self.TX_locs[self.TX_idx])) + torch.norm((self.RX_locs[self.RX_idx] - S_loc))\n",
    "                # rand_ph = torch.rand(1) * 2 * torch.pi\n",
    "                # rand_ph = torch.tensor(torch.pi/6)\n",
    "                phase = L[i]\n",
    "                A[i+1, i+1] = ((torch.exp(-2*torch.pi*(r/self.lam)*1j))*(torch.exp(phase*1j)))/r\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0994bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rate(H, sigma):\n",
    "    n_r = H.shape[0]\n",
    "    I = torch.eye(n_r, dtype=H.dtype)\n",
    "    M = I + H @ sigma @ H.conj().T\n",
    "    rate = torch.log2(torch.det(M))\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a723439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_erg_rate(num_samples, sigma):\n",
    "    erg_rate = 0\n",
    "    for _ in num_samples:\n",
    "        Br = p.calculate_Br()\n",
    "        Bt = p.calculate_Bt()\n",
    "        A = p.calculate_A()\n",
    "        H = Br @ A @ Bt.conj().T\n",
    "        erg_rate += calculate_rate(H, sigma)\n",
    "    return erg_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a888815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41f17fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2+3j, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c197d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.-4.5000j)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(2+1j, requires_grad=True)\n",
    "\n",
    "# Define the function f(x) = x^2\n",
    "f = x**3\n",
    "\n",
    "g = f.real\n",
    "\n",
    "# Perform backpropagation\n",
    "g.backward()\n",
    ".5* x.grad.conj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bf70567c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.-1.j)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(2+1j, requires_grad=True)\n",
    "\n",
    "# Define the function f(x) = x^2\n",
    "f = x*x.conj()\n",
    "\n",
    "g = f.real\n",
    "\n",
    "# Perform backpropagation\n",
    "g.backward()\n",
    ".5* x.grad.conj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c7c8b3eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.+1.j)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(z):\n",
    "    return z * z.conj()\n",
    "\n",
    "x = torch.tensor(3+1j, requires_grad=True)\n",
    "\n",
    "y = f(x).real\n",
    "y.backward()\n",
    "0.5*x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6bd86d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.7000+2.7000j, requires_grad=True)\n",
      "tensor(2.4300+2.4300j, requires_grad=True)\n",
      "tensor(2.1870+2.1870j, requires_grad=True)\n",
      "tensor(1.9683+1.9683j, requires_grad=True)\n",
      "tensor(1.7715+1.7715j, requires_grad=True)\n",
      "tensor(1.5943+1.5943j, requires_grad=True)\n",
      "tensor(1.4349+1.4349j, requires_grad=True)\n",
      "tensor(1.2914+1.2914j, requires_grad=True)\n",
      "tensor(1.1623+1.1623j, requires_grad=True)\n",
      "tensor(1.0460+1.0460j, requires_grad=True)\n",
      "tensor(0.9414+0.9414j, requires_grad=True)\n",
      "tensor(0.8473+0.8473j, requires_grad=True)\n",
      "tensor(0.7626+0.7626j, requires_grad=True)\n",
      "tensor(0.6863+0.6863j, requires_grad=True)\n",
      "tensor(0.6177+0.6177j, requires_grad=True)\n",
      "tensor(0.5559+0.5559j, requires_grad=True)\n",
      "tensor(0.5003+0.5003j, requires_grad=True)\n",
      "tensor(0.4503+0.4503j, requires_grad=True)\n",
      "tensor(0.4053+0.4053j, requires_grad=True)\n",
      "tensor(0.3647+0.3647j, requires_grad=True)\n",
      "tensor(0.3283+0.3283j, requires_grad=True)\n",
      "tensor(0.2954+0.2954j, requires_grad=True)\n",
      "tensor(0.2659+0.2659j, requires_grad=True)\n",
      "tensor(0.2393+0.2393j, requires_grad=True)\n",
      "tensor(0.2154+0.2154j, requires_grad=True)\n",
      "tensor(0.1938+0.1938j, requires_grad=True)\n",
      "tensor(0.1744+0.1744j, requires_grad=True)\n",
      "tensor(0.1570+0.1570j, requires_grad=True)\n",
      "tensor(0.1413+0.1413j, requires_grad=True)\n",
      "tensor(0.1272+0.1272j, requires_grad=True)\n",
      "tensor(0.1145+0.1145j, requires_grad=True)\n",
      "tensor(0.1030+0.1030j, requires_grad=True)\n",
      "tensor(0.0927+0.0927j, requires_grad=True)\n",
      "tensor(0.0834+0.0834j, requires_grad=True)\n",
      "tensor(0.0751+0.0751j, requires_grad=True)\n",
      "tensor(0.0676+0.0676j, requires_grad=True)\n",
      "tensor(0.0608+0.0608j, requires_grad=True)\n",
      "tensor(0.0547+0.0547j, requires_grad=True)\n",
      "tensor(0.0493+0.0493j, requires_grad=True)\n",
      "tensor(0.0443+0.0443j, requires_grad=True)\n",
      "tensor(0.0399+0.0399j, requires_grad=True)\n",
      "tensor(0.0359+0.0359j, requires_grad=True)\n",
      "tensor(0.0323+0.0323j, requires_grad=True)\n",
      "tensor(0.0291+0.0291j, requires_grad=True)\n",
      "tensor(0.0262+0.0262j, requires_grad=True)\n",
      "tensor(0.0236+0.0236j, requires_grad=True)\n",
      "tensor(0.0212+0.0212j, requires_grad=True)\n",
      "tensor(0.0191+0.0191j, requires_grad=True)\n",
      "tensor(0.0172+0.0172j, requires_grad=True)\n",
      "tensor(0.0155+0.0155j, requires_grad=True)\n",
      "tensor(0.0139+0.0139j, requires_grad=True)\n",
      "tensor(0.0125+0.0125j, requires_grad=True)\n",
      "tensor(0.0113+0.0113j, requires_grad=True)\n",
      "tensor(0.0101+0.0101j, requires_grad=True)\n",
      "tensor(0.0091+0.0091j, requires_grad=True)\n",
      "tensor(0.0082+0.0082j, requires_grad=True)\n",
      "tensor(0.0074+0.0074j, requires_grad=True)\n",
      "tensor(0.0067+0.0067j, requires_grad=True)\n",
      "tensor(0.0060+0.0060j, requires_grad=True)\n",
      "tensor(0.0054+0.0054j, requires_grad=True)\n",
      "tensor(0.0049+0.0049j, requires_grad=True)\n",
      "tensor(0.0044+0.0044j, requires_grad=True)\n",
      "tensor(0.0039+0.0039j, requires_grad=True)\n",
      "tensor(0.0035+0.0035j, requires_grad=True)\n",
      "tensor(0.0032+0.0032j, requires_grad=True)\n",
      "tensor(0.0029+0.0029j, requires_grad=True)\n",
      "tensor(0.0026+0.0026j, requires_grad=True)\n",
      "tensor(0.0023+0.0023j, requires_grad=True)\n",
      "tensor(0.0021+0.0021j, requires_grad=True)\n",
      "tensor(0.0019+0.0019j, requires_grad=True)\n",
      "tensor(0.0017+0.0017j, requires_grad=True)\n",
      "tensor(0.0015+0.0015j, requires_grad=True)\n",
      "tensor(0.0014+0.0014j, requires_grad=True)\n",
      "tensor(0.0012+0.0012j, requires_grad=True)\n",
      "tensor(0.0011+0.0011j, requires_grad=True)\n",
      "tensor(0.0010+0.0010j, requires_grad=True)\n",
      "tensor(0.0009+0.0009j, requires_grad=True)\n",
      "tensor(0.0008+0.0008j, requires_grad=True)\n",
      "tensor(0.0007+0.0007j, requires_grad=True)\n",
      "tensor(0.0007+0.0007j, requires_grad=True)\n",
      "tensor(0.0006+0.0006j, requires_grad=True)\n",
      "tensor(0.0005+0.0005j, requires_grad=True)\n",
      "tensor(0.0005+0.0005j, requires_grad=True)\n",
      "tensor(0.0004+0.0004j, requires_grad=True)\n",
      "tensor(0.0004+0.0004j, requires_grad=True)\n",
      "tensor(0.0003+0.0003j, requires_grad=True)\n",
      "tensor(0.0003+0.0003j, requires_grad=True)\n",
      "tensor(0.0003+0.0003j, requires_grad=True)\n",
      "tensor(0.0003+0.0003j, requires_grad=True)\n",
      "tensor(0.0002+0.0002j, requires_grad=True)\n",
      "tensor(0.0002+0.0002j, requires_grad=True)\n",
      "tensor(0.0002+0.0002j, requires_grad=True)\n",
      "tensor(0.0002+0.0002j, requires_grad=True)\n",
      "tensor(0.0001+0.0001j, requires_grad=True)\n",
      "tensor(0.0001+0.0001j, requires_grad=True)\n",
      "tensor(0.0001+0.0001j, requires_grad=True)\n",
      "tensor(0.0001+0.0001j, requires_grad=True)\n",
      "tensor(9.8376e-05+9.8376e-05j, requires_grad=True)\n",
      "tensor(8.8538e-05+8.8538e-05j, requires_grad=True)\n",
      "tensor(7.9684e-05+7.9684e-05j, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "def f(z):\n",
    "    return (z * z.conj()).real\n",
    "\n",
    "x = torch.tensor(3+3j, requires_grad=True)\n",
    "lr = 0.1\n",
    "\n",
    "for _ in range(100):\n",
    "    # 1) zero the old gradient\n",
    "    if x.grad is not None:\n",
    "        x.grad.zero_()\n",
    "\n",
    "    # 2) forward / backward\n",
    "    y = f(x)\n",
    "    y.backward()\n",
    "\n",
    "    # 3) extract Wirtinger derivative\n",
    "    grad = 0.5 * x.grad   # now x.grad is guaranteed not None\n",
    "\n",
    "    # 4) gradient step in-place, no_grad so we stay a leaf\n",
    "    with torch.no_grad():\n",
    "        x -= lr * grad\n",
    "\n",
    "    # x is already a leaf with requires_grad=True\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "53b1cd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.7000+1.1000j, requires_grad=True)\n",
      "tensor(2.4300+1.2100j, requires_grad=True)\n",
      "tensor(2.1870+1.3310j, requires_grad=True)\n",
      "tensor(1.9683+1.4641j, requires_grad=True)\n",
      "tensor(1.7715+1.6105j, requires_grad=True)\n",
      "tensor(1.5943+1.7716j, requires_grad=True)\n",
      "tensor(1.4349+1.9487j, requires_grad=True)\n",
      "tensor(1.2914+2.1436j, requires_grad=True)\n",
      "tensor(1.1623+2.3579j, requires_grad=True)\n",
      "tensor(1.0460+2.5937j, requires_grad=True)\n",
      "tensor(0.9414+2.8531j, requires_grad=True)\n",
      "tensor(0.8473+3.1384j, requires_grad=True)\n",
      "tensor(0.7626+3.4523j, requires_grad=True)\n",
      "tensor(0.6863+3.7975j, requires_grad=True)\n",
      "tensor(0.6177+4.1772j, requires_grad=True)\n",
      "tensor(0.5559+4.5950j, requires_grad=True)\n",
      "tensor(0.5003+5.0545j, requires_grad=True)\n",
      "tensor(0.4503+5.5599j, requires_grad=True)\n",
      "tensor(0.4053+6.1159j, requires_grad=True)\n",
      "tensor(0.3647+6.7275j, requires_grad=True)\n",
      "tensor(0.3283+7.4003j, requires_grad=True)\n",
      "tensor(0.2954+8.1403j, requires_grad=True)\n",
      "tensor(0.2659+8.9543j, requires_grad=True)\n",
      "tensor(0.2393+9.8497j, requires_grad=True)\n",
      "tensor(0.2154+10.8347j, requires_grad=True)\n",
      "tensor(0.1938+11.9182j, requires_grad=True)\n",
      "tensor(0.1744+13.1100j, requires_grad=True)\n",
      "tensor(0.1570+14.4210j, requires_grad=True)\n",
      "tensor(0.1413+15.8631j, requires_grad=True)\n",
      "tensor(0.1272+17.4494j, requires_grad=True)\n",
      "tensor(0.1145+19.1943j, requires_grad=True)\n",
      "tensor(0.1030+21.1138j, requires_grad=True)\n",
      "tensor(0.0927+23.2252j, requires_grad=True)\n",
      "tensor(0.0834+25.5477j, requires_grad=True)\n",
      "tensor(0.0751+28.1024j, requires_grad=True)\n",
      "tensor(0.0676+30.9127j, requires_grad=True)\n",
      "tensor(0.0608+34.0039j, requires_grad=True)\n",
      "tensor(0.0547+37.4043j, requires_grad=True)\n",
      "tensor(0.0493+41.1448j, requires_grad=True)\n",
      "tensor(0.0443+45.2593j, requires_grad=True)\n",
      "tensor(0.0399+49.7852j, requires_grad=True)\n",
      "tensor(0.0359+54.7637j, requires_grad=True)\n",
      "tensor(0.0323+60.2401j, requires_grad=True)\n",
      "tensor(0.0291+66.2641j, requires_grad=True)\n",
      "tensor(0.0262+72.8905j, requires_grad=True)\n",
      "tensor(0.0236+80.1795j, requires_grad=True)\n",
      "tensor(0.0212+88.1975j, requires_grad=True)\n",
      "tensor(0.0191+97.0172j, requires_grad=True)\n",
      "tensor(0.0172+106.7190j, requires_grad=True)\n",
      "tensor(0.0155+117.3909j, requires_grad=True)\n",
      "tensor(0.0139+129.1299j, requires_grad=True)\n",
      "tensor(0.0125+142.0429j, requires_grad=True)\n",
      "tensor(0.0113+156.2472j, requires_grad=True)\n",
      "tensor(0.0101+171.8720j, requires_grad=True)\n",
      "tensor(0.0091+189.0592j, requires_grad=True)\n",
      "tensor(0.0082+207.9651j, requires_grad=True)\n",
      "tensor(0.0074+228.7616j, requires_grad=True)\n",
      "tensor(0.0067+251.6377j, requires_grad=True)\n",
      "tensor(0.0060+276.8015j, requires_grad=True)\n",
      "tensor(0.0054+304.4817j, requires_grad=True)\n",
      "tensor(0.0049+334.9298j, requires_grad=True)\n",
      "tensor(0.0044+368.4228j, requires_grad=True)\n",
      "tensor(0.0039+405.2651j, requires_grad=True)\n",
      "tensor(0.0035+445.7916j, requires_grad=True)\n",
      "tensor(0.0032+490.3708j, requires_grad=True)\n",
      "tensor(0.0029+539.4078j, requires_grad=True)\n",
      "tensor(0.0026+593.3486j, requires_grad=True)\n",
      "tensor(0.0023+652.6835j, requires_grad=True)\n",
      "tensor(0.0021+717.9518j, requires_grad=True)\n",
      "tensor(0.0019+789.7470j, requires_grad=True)\n",
      "tensor(0.0017+868.7217j, requires_grad=True)\n",
      "tensor(0.0015+955.5939j, requires_grad=True)\n",
      "tensor(0.0014+1051.1533j, requires_grad=True)\n",
      "tensor(0.0012+1156.2687j, requires_grad=True)\n",
      "tensor(0.0011+1271.8955j, requires_grad=True)\n",
      "tensor(0.0010+1399.0851j, requires_grad=True)\n",
      "tensor(0.0009+1538.9937j, requires_grad=True)\n",
      "tensor(0.0008+1692.8931j, requires_grad=True)\n",
      "tensor(0.0007+1862.1824j, requires_grad=True)\n",
      "tensor(0.0007+2048.4006j, requires_grad=True)\n",
      "tensor(0.0006+2253.2407j, requires_grad=True)\n",
      "tensor(0.0005+2478.5647j, requires_grad=True)\n",
      "tensor(0.0005+2726.4211j, requires_grad=True)\n",
      "tensor(0.0004+2999.0632j, requires_grad=True)\n",
      "tensor(0.0004+3298.9695j, requires_grad=True)\n",
      "tensor(0.0003+3628.8665j, requires_grad=True)\n",
      "tensor(0.0003+3991.7532j, requires_grad=True)\n",
      "tensor(0.0003+4390.9287j, requires_grad=True)\n",
      "tensor(0.0003+4830.0215j, requires_grad=True)\n",
      "tensor(0.0002+5313.0234j, requires_grad=True)\n",
      "tensor(0.0002+5844.3257j, requires_grad=True)\n",
      "tensor(0.0002+6428.7583j, requires_grad=True)\n",
      "tensor(0.0002+7071.6343j, requires_grad=True)\n",
      "tensor(0.0001+7778.7979j, requires_grad=True)\n",
      "tensor(0.0001+8556.6777j, requires_grad=True)\n",
      "tensor(0.0001+9412.3457j, requires_grad=True)\n",
      "tensor(0.0001+10353.5801j, requires_grad=True)\n",
      "tensor(9.8376e-05+11388.9385j, requires_grad=True)\n",
      "tensor(8.8538e-05+12527.8320j, requires_grad=True)\n",
      "tensor(7.9684e-05+13780.6152j, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3+1j, requires_grad=True)\n",
    "lr = 0.1\n",
    "\n",
    "for _ in range(100):\n",
    "    if x.grad is not None:\n",
    "        x.grad.zero_()\n",
    "\n",
    "    y = (x * x.conj()).real\n",
    "    y.backward()\n",
    "\n",
    "    # ⟵ Take the conjugate so we step along ∂f/∂z* = z\n",
    "    grad = 0.5 * x.grad.conj()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x -= lr * grad\n",
    "\n",
    "    # x is still a leaf with retains its .grad buffer\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4d4898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fixed channel\n",
    "#Setting Network Parameters\n",
    "num_RX_ant = 2\n",
    "num_TX_ant = 4\n",
    "num_scatterers = torch.tensor([[1, 1], [1, 1]])\n",
    "lam = 0.01\n",
    "Ant_dist = 0.005\n",
    "#Setting Locations\n",
    "TX_locs = [torch.tensor([-10.0, 0.0], dtype=torch.float64), torch.tensor([0.0, 0.0], dtype=torch.float64)]\n",
    "RX_locs = [torch.tensor([-10.0, -10.0], dtype=torch.float64), torch.tensor([0, -10.0], dtype=torch.float64)]\n",
    "SC_locs = np.array([[torch.tensor([[-20.0, -5.0]], dtype=torch.float64), torch.tensor([[10.0, -5.0]], dtype=torch.float64)],\n",
    "                     [torch.tensor([[-20.0, -5.0]], dtype=torch.float64), torch.tensor([[10.0, -5.0]], dtype=torch.float64)]])\n",
    "\n",
    "N = Network()\n",
    "N.num_RX_ant = num_RX_ant\n",
    "N.num_TX_ant = num_TX_ant\n",
    "N.num_scatterers = num_scatterers\n",
    "N.lam = lam\n",
    "N.Ant_dist = Ant_dist\n",
    "N.TX_locs = TX_locs\n",
    "N.RX_locs = RX_locs\n",
    "N.SC_locs = SC_locs\n",
    "N.rand_ph = 1\n",
    "\n",
    "p = pair(N, 0, 1)\n",
    "\n",
    "Br = p.calculate_Br()\n",
    "Bt = p.calculate_Bt()\n",
    "A = p.calculate_A()\n",
    "H = Br @ A @ Bt.conj().T\n",
    "\n",
    "\n",
    "Sigma0 = torch.eye(4)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_pCSI_Beamforming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
